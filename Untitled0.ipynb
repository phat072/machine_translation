{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+X16nvggExezXfCVUMgTA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["ref = {'Seq2Seq': 'Liu, T., Wang, K., Sha, L., Chang, B., & Sui, Z. (2017). Table-to-text Generation by Structure-aware Seq2seq Learning. AAAI Conference on Artificial Intelligence.',\n"," 'PhoMT': 'Doan, L., Nguyen, L.T., Tran, N.L., Hoang, T.Q., & Nguyen, D.Q. (2021). PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation. ArXiv, abs/2110.12199.',\n"," 'NMT_Nguyen': 'Nguyen, T.H., Phung, D., Nguyen, D.T., Tran, H.M., Luong, M., Vo, T., Bui, H.H., Phung, D., & Nguyen, D.Q. (2022). A Vietnamese-English Neural Machine Translation System. Interspeech.',\n"," 'MT_Phan_2018': 'Phan-Vu, H., Tran, V., Nguyen, V., Dang, H., & Do, P. (2018). Machine Translation between Vietnamese and English: an Empirical Study. ArXiv, abs/1810.12557.',\n"," 'BART': 'Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Annual Meeting of the Association for Computational Linguistics.',\n"," 'T5': 'Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. ArXiv, abs/1910.10683.',\n"," 'BLEU': 'Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2002). Bleu: a Method for Automatic Evaluation of Machine Translation. Annual Meeting of the Association for Computational Linguistics.',\n"," 'VLSP2022': 'Nguyen, V.V., Nguyen, H., Le, H.T., Nguyen, T.P., Bui, T.V., Pham, L.N., Phan, A., Nguyen, C., Tran, V., & Tran, A. (2022). KC4MT: A High-Quality Corpus for Multilingual Machine Translation. International Conference on Language Resources and Evaluation.',\n"," 'Character_Tran_2016': 'Tran, P., Dien, D., & Nguyen, H.T. (2016). A Character Level Based and Word Level Based Approach for Chinese-Vietnamese Machine Translation. Computational Intelligence and Neuroscience, 2016.',\n"," 'BT_LI_2020': 'Li, H., & Huang, H. (2020). Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation. ArXiv, abs/2003.02197.',\n"," 'Mono_Burlot_2018': 'Burlot, F., & Yvon, F. (2018). Using Monolingual Data in Neural Machine Translation: a Systematic Study. ArXiv, abs/1903.11437.',\n"," 'Wen2023ChineseVietnamesePS': 'Wen, Y., Guo, J., Yu, Z., & Yu, Z. (2023). Chinese-Vietnamese Pseudo-Parallel Sentences Extraction Based on Image Information Fusion. Inf., 14, 298.',\n"," 'Quan2021ViNMTNM': 'Quan, N.H., Dat, N.T., Hoang, M., Vinh, N.V., Vinh, N.T., Thai, N.P., & Viet, T.H. (2021). ViNMT: Neural Machine Translation Tookit. ArXiv, abs/2112.15272.',\n"," 'Ranathunga2021NeuralMT': 'Ranathunga, S., Lee, E.A., Skenduli, M.P., Shekhar, R., Alam, M., & Kaur, R. (2021). Neural Machine Translation for Low-resource Languages: A Survey. ACM Computing Surveys, 55, 1 - 37.',\n"," 'Wolf2019HuggingFacesTS': \"Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., & Brew, J. (2019). HuggingFace's Transformers: State-of-the-art Natural Language Processing. ArXiv, abs/1910.03771.\",\n"," 'mBart': 'Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L. (2020). Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8, 726-742.',\n"," 'mT5': 'Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2020). mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. North American Chapter of the Association for Computational Linguistics.',\n"," 'ByT5': 'Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., & Raffel, C. (2021). ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10, 291-306.',\n"," 'Marian': 'Junczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T., Hoang, H.T., Heafield, K., Neckermann, T., Seide, F., Germann, U., Aji, A., Bogoychev, N., Martins, A.F., & Birch, A. (2018). Marian: Fast Neural Machine Translation in C++. ArXiv, abs/1804.00344.',\n"," 'BPEmb': 'Heinzerling, B., & Strube, M. (2017). BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages. ArXiv, abs/1710.02187.',\n"," 'Moses': 'Koehn, P., Hoang, H.T., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E. (2007). Moses: Open Source Toolkit for Statistical Machine Translation. Annual Meeting of the Association for Computational Linguistics.',\n"," 'SacreBLEU': 'Post, M. (2018). A Call for Clarity in Reporting BLEU Scores. Conference on Machine Translation.',\n"," 'Colab': 'Carneiro, T., Medeiros Da Nóbrega, R.V., Nepomuceno, T., Bian, G., de Albuquerque, V.H., & Filho, P. (2018). Performance Analysis of Google Colaboratory as a Tool for Accelerating Deep Learning Applications. IEEE Access, 6, 61677-61685.'}"],"metadata":{"id":"8X51LRtOS31w","executionInfo":{"status":"ok","timestamp":1687977630884,"user_tz":-420,"elapsed":508,"user":{"displayName":"ĐẠI NGUYỄN BÁ","userId":"16481774491770005947"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["corpus = input()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4Q25bAWSqdw","executionInfo":{"status":"ok","timestamp":1687979396212,"user_tz":-420,"elapsed":1643,"user":{"displayName":"ĐẠI NGUYỄN BÁ","userId":"16481774491770005947"}},"outputId":"d126d2e8-6f02-4fe0-d91e-7fbb25906580"},"execution_count":54,"outputs":[{"name":"stdout","output_type":"stream","text":["THÔNG TIN KẾT QUẢ NGHIÊN CỨU  Thông tin chung:  1.1. Mục tiêu nghiên cứu Nghiên cứu nhằm xây dựng một phương pháp hiệu quả cho bài toán dịch máy Việt-Trung trong trường hợp có giới hạn dữ liệu, thông qua kết hợp các phương pháp tiền xử lý văn bản tiếng Việt và tiếng Trung cùng với các mô hình ngôn ngữ hiện đại như BART \\cite{BART} và T5 \\cite{T5}.  1.2. Tiến trình nghiên cứu Nghiên cứu được tiến hành bằng cách thử nghiệm và đánh giá các phương pháp tiền xử lý văn bản như tách từ, tách từ đồng tham chiếu, và chuẩn hóa câu trước khi áp dụng các mô hình dịch máy. Các mô hình ngôn ngữ hiện đại như BART và T5 cũng được sử dụng và đánh giá về hiệu suất và độ chính xác.  1.3. Kết quả nghiên cứu Kết quả nghiên cứu nhằm tạo ra một phương pháp dịch máy Việt - Trung hiệu quả trong trường hợp có giới hạn dữ liệu. Các kết quả đạt được được đánh giá dựa trên độ chính xác của các bản dịch và hiệu suất của các mô hình dịch máy.  1.4. Đóng góp của nghiên cứu Nghiên cứu này đóng góp bằng việc nghiên cứu và áp dụng các phương pháp tiền xử lý văn bản và mô hình ngôn ngữ để cải thiện hiệu suất dịch máy Việt - Trung. Kết quả của nghiên cứu có thể đóng vai trò là nền tảng cho phát triển tiếp theo trong lĩnh vực này.  1.5. Hướng phát triển tiếp theo Các nghiên cứu tiếp theo có thể tập trung vào cải thiện hiệu suất dịch máy Việt - Trung bằng cách sử dụng phương pháp tăng cường dữ liệu, khai thác thông tin từ nguồn dữ liệu ngoại vi, hoặc nghiên cứu về các mô hình dịch máy khác như Transformer để so sánh và tối ưu hóa hiệu suất dịch.    Mục tiêu Mục tiêu của thông tin kết quả nghiên cứu là cung cấp những thông tin chung về kết quả nghiên cứu về dự án dịch máy Việt - Trung. Thông qua việc trình bày các thông tin về mục tiêu, tiến trình, kết quả, đóng góp và hướng phát triển tiếp theo của nghiên cứu, mục tiêu là giúp người đọc có cái nhìn tổng quan về quá trình nghiên cứu và những kết quả đạt được trong lĩnh vực dịch máy Việt - Trung. Tính mới và tính sáng tạo 3.1. Tính mới Sử dụng các phương pháp mới: Nghiên cứu có thể đề xuất và áp dụng các phương pháp mới, thuật toán mới hoặc công nghệ mới trong lĩnh vực dịch máy Việt - Trung. Điều này đảm bảo rằng thông tin kết quả nghiên cứu mang tính đột phá và khác biệt so với các nghiên cứu trước đây. Cung cấp các phát hiện mới: Thông tin kết quả nghiên cứu có thể tiết lộ những phát hiện mới, những tri thức mới về cách cải thiện dịch máy Việt - Trung. Điều này có thể bao gồm việc tìm ra các mô hình ngôn ngữ mới, quy tắc ngữ pháp mới hoặc phát hiện các đặc trưng ngôn ngữ mới giúp cải thiện hiệu suất dịch. 3.2 Tính sáng tạo Cung cấp các giải pháp độc đáo: Thông tin kết quả nghiên cứu có thể đề xuất giải pháp độc đáo và sáng tạo để giải quyết các vấn đề khó khăn trong dịch máy Việt - Trung. Điều này có thể bao gồm việc tạo ra các kỹ thuật mới để xử lý các khía cạnh đặc thù của ngôn ngữ Việt và Trung, như sự đa nghĩa, cấu trúc ngữ pháp phức tạp hoặc khái niệm văn hóa. Tính ứng dụng sáng tạo: Thông tin kết quả nghiên cứu có thể đề xuất và phát triển các ứng dụng sáng tạo trong lĩnh vực dịch máy Việt - Trung. Ví dụ, tạo ra các công cụ dịch máy dựa trên trí tuệ nhân tạo, tích hợp dịch máy vào các hệ thống thông tin tự động hoặc phát triển giao diện người dùng độc đáo để cải thiện trải nghiệm dịch. Tóm tắt kết quả nghiên cứu Sau khi khảo sát các mô hình dạng mBart \\cite{mBart}, MarianMT \\cite{Marian}, và T5 như mT5 \\cite{mT5}, ByT5 \\cite{ByT5}, nhóm chúng tôi đưa ra các kết luận sau: Mô hình mBart cho hiệu suất vượt trội hơn hẳn so với các mô hình MarianMT, và T5 trong bài toán dịch máy. Cấu trúc mô hình many-many cho hiệu quả tốt hơn rất nhiều so với các cấu trúc khác và chúng tôi cũng đưa ra các mức độ hiệu quả của các cấu trúc như sau: many-many > many-one > one-many > one-one  Tóm lại, qua nghiên cứu khoa học này, nhóm chúng tôi đã khảo sát được độ hiệu quả của các mô hình, mối liên hệ giữa các thông số với độ hiệu quả của mô hình, các cấu trúc input_ouput hiệu quả của mô hình cũng như cách để tối ưu tài nguyên sử dụng khi huấn luyện mô hình. Tuy nhiên chúng tôi vẫn chưa thể khảo sát được mô hình google/byt5-small do sự thiếu hụt về tài nguyên nhưng dựa vào nhũng nghiên cứu, khảo sát trên, chúng tôi tin rằng mô hình dạng mBart vẫn sẽ cho hiệu quả cao nhất và vượt trội so với các dạng mô hình khác. Tên sản phẩm NGHIÊN CỨU PHƯƠNG PHÁP DỊCH MÁY VIỆT - TRUNG DỰA TRÊN CÁC MÔ HÌNH NGÔN NGỮ Hiệu quả, phương thức chuyển giao kết quả nghiên cứu và khả năng áp dụng 6.1. Hiệu quả Đề tài nghiên cứu \"Dịch máy Việt - Trung dựa trên mô hình ngôn ngữ\" đã mang lại những kết quả đáng kể và hiệu quả trong lĩnh vực dịch máy tiếng Việt - tiếng Trung. Nhờ sự nghiên cứu và phát triển trong đề tài, chúng tôi đã đạt được những thành tựu quan trọng, góp phần mở ra những tiềm năng và cơ hội mới cho việc cải thiện hiệu suất và chất lượng của các hệ thống dịch máy. Qua quá trình nghiên cứu, chúng tôi đã áp dụng các phương pháp tiền xử lý văn bản và sử dụng các mô hình ngôn ngữ tiên tiến như mBART và mT5, nhằm nâng cao khả năng dịch máy, đảm bảo sự chính xác và tự nhiên trong quá trình chuyển đổi từ ngôn ngữ Việt sang Trung và ngược lại. Kết quả nghiên cứu đã chứng minh rằng các phương pháp và kỹ thuật được áp dụng đã cải thiện đáng kể hiệu suất và chất lượng của hệ thống dịch máy. 6.2. Phương thức chuyển giao kết quả nghiên cứu Các kết quả nghiên cứu có thể được chuyển giao thông qua nhiều phương thức khác nhau. Một phương thức chuyển giao phổ biến là công bố bài báo khoa học trong các hội nghị hoặc tạp chí chuyên ngành. Bằng cách công bố kết quả nghiên cứu, nhóm nghiên cứu có thể chia sẻ kiến thức và phương pháp đã sử dụng với cộng đồng nghiên cứu và các nhà quản lý ngành công nghiệp liên quan. Thêm nữa, việc xây dựng một ứng dụng dịch máy dựa trên kết quả nghiên cứu cũng là một cách chuyển giao hiệu quả. Bằng cách phát triển một ứng dụng thực tế và cung cấp nó cho người dùng, nhóm nghiên cứu có thể áp dụng kết quả nghiên cứu của mình vào việc giải quyết các vấn đề thực tiễn.  6.3. Khả năng áp dụng Kết quả nghiên cứu có khả năng áp dụng rộng rãi trong lĩnh vực dịch máy tiếng Việt - tiếng Trung. Các phương pháp tiền xử lý văn bản và mô hình ngôn ngữ hiện đại như mBART và mT5 có thể được sử dụng để cải thiện hiệu suất và chất lượng của các hệ thống dịch máy hiện có. Ngoài ra, kết quả nghiên cứu này cũng có thể được áp dụng trong việc phát triển các ứng dụng văn bản tự động khác như tổng hợp tin tức, trích xuất thông tin, hoặc phân loại văn bản. Các phương pháp và kỹ thuật đã được sử dụng có thể được tùy chỉnh và áp dụng cho các vấn đề văn bản khác nhau để giúp cải thiện hiệu suất và độ chính xác.\n"]}]},{"cell_type":"code","source":["sent = [i for i in corpus.split(\".\") if \"\\cite\" in i ]\n","sent_pp = []\n","for i in sent:\n","    sent_pp += [\"\\\\cite\" + j for j in i.split(\"\\\\cite\")]\n","\n","sent_pp = [i for i in sent_pp if \"\\\\cite{\" in i]\n","sent_pp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hD-IMUrBTqtP","executionInfo":{"status":"ok","timestamp":1687979398809,"user_tz":-420,"elapsed":314,"user":{"displayName":"ĐẠI NGUYỄN BÁ","userId":"16481774491770005947"}},"outputId":"3dbf2c30-32fc-4a2a-fd6c-d622b4597581"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\\\cite{BART} và T5 ',\n"," '\\\\cite{T5}',\n"," '\\\\cite{mBart}, MarianMT ',\n"," '\\\\cite{Marian}, và T5 như mT5 ',\n"," '\\\\cite{mT5}, ByT5 ',\n"," '\\\\cite{ByT5}, nhóm chúng tôi đưa ra các kết luận sau: Mô hình mBart cho hiệu suất vượt trội hơn hẳn so với các mô hình MarianMT, và T5 trong bài toán dịch máy']"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["import re\n","\n","cite_re = r\"\\\\cite\\{([^\\s]+)\\}\"\n","\n","cite = {}\n","for i in sent_pp:\n","    keys = re.search(cite_re, i)[1].split(\",\")\n","    for key in keys:\n","        if key not in cite:\n","            cite[key] = ref[key]\n","\n","cite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcJ58F6fUL__","executionInfo":{"status":"ok","timestamp":1687979406486,"user_tz":-420,"elapsed":355,"user":{"displayName":"ĐẠI NGUYỄN BÁ","userId":"16481774491770005947"}},"outputId":"c5a7fb5f-a4f9-433c-d829-7dc94605c059"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'BART': 'Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Annual Meeting of the Association for Computational Linguistics.',\n"," 'T5': 'Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. ArXiv, abs/1910.10683.',\n"," 'mBart': 'Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L. (2020). Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8, 726-742.',\n"," 'Marian': 'Junczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T., Hoang, H.T., Heafield, K., Neckermann, T., Seide, F., Germann, U., Aji, A., Bogoychev, N., Martins, A.F., & Birch, A. (2018). Marian: Fast Neural Machine Translation in C++. ArXiv, abs/1804.00344.',\n"," 'mT5': 'Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2020). mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. North American Chapter of the Association for Computational Linguistics.',\n"," 'ByT5': 'Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., & Raffel, C. (2021). ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10, 291-306.'}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["for idx, key in enumerate(cite):\n","    # print(key, idx + 1)\n","    print(f\"[{idx + 1}]. {cite[key]}\")\n","\n","for idx, key in enumerate(cite):\n","    print(key)\n","    print(idx + 1)\n","    # print(f\"[{idx + 1}]. {cite[key]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QFjui1gWEyw","executionInfo":{"status":"ok","timestamp":1687979408979,"user_tz":-420,"elapsed":2,"user":{"displayName":"ĐẠI NGUYỄN BÁ","userId":"16481774491770005947"}},"outputId":"d984c10b-91f5-4b19-d0aa-dfbaa3552e66"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["[1]. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Annual Meeting of the Association for Computational Linguistics.\n","[2]. Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. ArXiv, abs/1910.10683.\n","[3]. Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., & Zettlemoyer, L. (2020). Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8, 726-742.\n","[4]. Junczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T., Hoang, H.T., Heafield, K., Neckermann, T., Seide, F., Germann, U., Aji, A., Bogoychev, N., Martins, A.F., & Birch, A. (2018). Marian: Fast Neural Machine Translation in C++. ArXiv, abs/1804.00344.\n","[5]. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2020). mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. North American Chapter of the Association for Computational Linguistics.\n","[6]. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., & Raffel, C. (2021). ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10, 291-306.\n","BART\n","1\n","T5\n","2\n","mBart\n","3\n","Marian\n","4\n","mT5\n","5\n","ByT5\n","6\n"]}]}]}